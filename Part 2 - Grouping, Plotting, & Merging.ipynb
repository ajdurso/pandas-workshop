{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Grouping\n",
    "\n",
    "The pandas `groupby` function allows us to group our data on the values in a column or column to look at summary measures for records sharing the same values.\n",
    "\n",
    "For example, let's load the speed camera dataset again and ask which camera locations or days of the week have produced the most violations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "df = pd.read_csv(\"data/Speed_Camera_Violations.csv\")\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that the data is loaded, let's find the 10 locations with the most total violations recorded.\n",
    "\n",
    "To do this, we need to group by the ADDRESS column, then examine the VIOLATIONS column of the resulting grouped dataframe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# first let's group by address and look at descriptive statistics for the first 10 records\n",
    "df.groupby([\"ADDRESS\"])[\"VIOLATIONS\"].describe().head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The above records aren't sorted in any meaningful way, but the first thing to note is that the Index is no longer just an integer, it is now the Address. This is because the `groupby` method returns a special object with a new index made up of the \n",
    "values of the column being grouped on.\n",
    "\n",
    "We can still use the `loc` indexer with this new grouped object to, for example, find the count for a given address:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# `count` returns the number of rows for this address, not the total violation count.\n",
    "# IE this tells us the number of observations.\n",
    "df.groupby([\"ADDRESS\"])[\"VIOLATIONS\"].count().loc[\"19 W CHICAGO AVE\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# to get the total violation count, we want the `sum` method:\n",
    "df.groupby([\"ADDRESS\"])[\"VIOLATIONS\"].sum().loc[\"19 W CHICAGO AVE\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now let's get the top 10 camera locations by total violation count:\n",
    "df.groupby([\"ADDRESS\"])[\"VIOLATIONS\"].sum().sort_values(ascending=False).head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It's possible that some locations just have more observations than others, so a more meaningful measure is probably the mean violation count per observation. To get this we just need to use the `mean` function rather than `sum`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.groupby([\"ADDRESS\"])[\"VIOLATIONS\"].mean().sort_values(ascending=False).head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "How about days of the week? *When* are people most likely to be caught speeding?\n",
    "\n",
    "The simplest way to do this is to create a new weekday column and group on that."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# datetime series have a special `dt` property that exposes the date/time-specific functionality.\n",
    "# In this case, dayofweek is a 0-based index where 0 = Monday, 6 = Sunday.\n",
    "df[\"VIOLATION DATE\"] = pd.to_datetime(df[\"VIOLATION DATE\"], format=\"%m/%d/%Y\")\n",
    "df[\"VIOLATION DATE\"].dt.dayofweek.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\"DAY OF WEEK\"] = df[\"VIOLATION DATE\"].dt.dayofweek\n",
    "df.groupby([\"DAY OF WEEK\"])[\"VIOLATIONS\"].mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Plotting\n",
    "\n",
    "It's not easy to understand at a glance the distribution of speeding violations by day of the week above, so let's produce a simple plot to visualize and help understand it.\n",
    "\n",
    "Pandas has a number of convenience functions to let us output plots directly without having to interact with matplotlib."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# a bit of jupyter-specific magic\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We can call `plot` on just about anything to get a minimally-formatted plot\n",
    "df.groupby([\"DAY OF WEEK\"])[\"VIOLATIONS\"].mean().plot()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can do much better. Let's do a horizontal bar plot, renaming the labels to the actual days of the week, and starting the X-axis at 0 to give a better sense of how much this data actually varies."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a mapping of day number to day name\n",
    "daynames = {0: \"Mon\", 1: \"Tue\", 2: \"Wed\", 3: \"Thu\", 4: \"Fri\", 5: \"Sat\", 6: \"Sun\"}\n",
    "\n",
    "# save the \"sum of violations grouped-by day of week\" series in a variable\n",
    "violations_by_day_of_week = df.groupby([\"DAY OF WEEK\"])[\"VIOLATIONS\"].mean()\n",
    "\n",
    "# use the mapping created above to give the series index labels sensible values\n",
    "violations_by_day_of_week.rename(index=daynames, inplace=True)\n",
    "\n",
    "# use the reindex method to order them the way we want in the plot (starting with Sunday at the top of the y axis)\n",
    "violations_by_day_of_week = violations_by_day_of_week.reindex([\"Sat\", \"Fri\", \"Thu\", \"Wed\", \"Tue\", \"Mon\", \"Sun\"])\n",
    "\n",
    "# and finally, create the bar plot\n",
    "violations_by_day_of_week.plot(kind='barh', title=\"City of Chicago Speed Camera Violations By Day Of Week\") "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Combining DataFrames\n",
    "\n",
    "Often you will need to combine data from multiple data sets together. There are two types of combinations: concatenations and merges (aka joins).\n",
    "\n",
    "**Concatenating** means taking multiple DataFrame objects and appending their rows together to make a new DataFrame. In general you will do this when your datasets contain the same columns and you are combining observations of the same type together into one dataset that contains all the rows from all the datasets.\n",
    "\n",
    "**Merging** is joining DataFrames together SQL-style by using common values. This is useful when you have multiple datasets with common keys and you want to combine them into one dataset that contains columns from all the datasets being merged."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Concatenation example\n",
    "df1 = pd.DataFrame({'A': [1, 2, 3],\n",
    "                    'B': [4, 5, 6]})\n",
    "\n",
    "df2 = pd.DataFrame({'A': [7, 8, 9],\n",
    "                    'B': [10, 11, 12]})\n",
    "\n",
    "print(\"df1: \")\n",
    "print(df1)\n",
    "print()\n",
    "print(\"df2: \")\n",
    "print(df2)\n",
    "print()\n",
    "print(\"concatenated: \")\n",
    "print(pd.concat([df1, df2]))\n",
    "df3 = pd.concat([df1, df2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Merge example\n",
    "df1 = pd.DataFrame({'A': [1, 2, 3],\n",
    "                    'B': [4, 5, 6]})\n",
    "\n",
    "df2 = pd.DataFrame({'A': [1, 2, 3],\n",
    "                    'C': [\"foo\", \"bar\", \"baz\"]})\n",
    "\n",
    "print(\"df1: \")\n",
    "print(df1)\n",
    "print()\n",
    "print(\"df2: \")\n",
    "print(df2)\n",
    "print()\n",
    "print(\"merged: \")\n",
    "print(pd.merge(df1, df2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hierarchical Indexing\n",
    "\n",
    "Up to now we have looked only at 1- and 2-dimensional data. While Pandas does offer objects for handling 3- and 4-dimesional data (the `Panel` and `Panel4D`), it's often more convenient to keep higher dimensional data in a `DataFrame` but use an index with multiple *levels*. This kind of index is called a `MultiIndex` in pandas.\n",
    "\n",
    "Let us use for example mean annual water level data for the great lakes for 2015 and 2016. We'd like to be able to look up, by year and lake, the mean water level, in a single series. To do this, we can just create the series as before, but pass in a list of lists for the index."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = [183.57, 183.58, 176.59, 176.70, 175.20, 175.35, 174.32, 174.41, 74.74, 74.80]\n",
    "\n",
    "\n",
    "labels = [\n",
    "    ['Superior', 'Superior',\n",
    "     'Michigan-Huron', 'Michigan-Huron',\n",
    "     'St. Clair', 'St. Clair',\n",
    "     'Erie', 'Erie',\n",
    "     'Ontario', 'Ontario'],\n",
    "    [2015, 2016,\n",
    "     2015, 2016,\n",
    "     2015, 2016,\n",
    "     2015, 2016,\n",
    "     2015, 2016,]\n",
    "]\n",
    "\n",
    "mean_levels = pd.Series(data, index=labels)\n",
    "mean_levels.index.rename(['Lake', 'Year'])\n",
    "mean_levels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We can now get data by Lake and Year:\n",
    "mean_levels['Superior', 2015]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Or do \"partial indexing\": select a label from one level of the index, getting back a series with all remaining levels:\n",
    "mean_levels['Superior']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# to do partial indexing on the \"inner\" level, we need to pass in an empty slice for the outer level:\n",
    "mean_levels[:, 2015]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "But why torture a series like this when this data could easily be represented in a DataFrame, with the lakes as columns?\n",
    "\n",
    "Because we can do this same thing with more series and combine them into a dataframe that shares this two-dimensional index! If you squint, it's higher-dimensional data!!\n",
    "\n",
    "We have a CSV containing monthly water level readings for the great lakes going back to 1918. We'll use it to calculate annual min and max readings, and then combine those calculated series with this series that contains annual means to create a new dataframe with year and lake as the index, and min, max, and mean as the columns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# the `index_col` parameter allows us to specify which columns to use as an index when creating\n",
    "# the dataframe and in which order. Very handy!\n",
    "df = pd.read_csv('data/GLHYD_data_metric.csv', header=12, index_col=[1, 0])\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "At this point we have the lakes as *columns*, when what we really want is to take all of those column names and add them to our index as another level. This is exactly what the very useful [`stack`](https://pandas.pydata.org/pandas-docs/stable/generated/pandas.DataFrame.stack.html) method does. It \"pivots\" a level of column labels as a new innermost index level.\n",
    "\n",
    "In this case, because our dataframe has only one level of column labels, `stack` turns that level into a new index level leaving us with only a series. We have \"moved a dimension\" and transformed a dataframe with a 2-d index to a series with a 3-d index."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lake_monthly_levels = df.stack()\n",
    "lake_monthly_levels.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now in order to make it easy for Pandas to combine series we want to give our index levels nice, consistent names."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lake_monthly_levels.index.set_names(['Year', 'Month', 'Lake'], inplace=True)\n",
    "lake_monthly_levels.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate the max water level for each year for each lake\n",
    "max_levels = lake_monthly_levels.groupby(['Year', 'Lake']).agg('max')\n",
    "max_levels.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "min_levels = lake_monthly_levels.groupby(['Year', 'Lake']).agg('min')\n",
    "min_levels.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If we were to try to merge these three series together now, Pandas would fail because the index levels aren't in the same order. So the `mean_levels` series needs to have the order of its index levels swapped so that \"Year\" is the outer level. This is easily done with the `reorder_levels` method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mean_levels = mean_levels.reorder_levels([1, 0])\n",
    "mean_levels.index.set_names(['Year', 'Lake'], inplace=True)\n",
    "mean_levels"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And now, finally, we can merge these three series together into a new dataframe (and drop all observations before 2015 that don't have). Note this is a very contrived example as it would have been trivially easy to calculate the mean water levels from the CSV we loaded :)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame({'Mean': mean_levels, 'Max': max_levels, 'Min': min_levels}).dropna()\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# to select values based on just the inner index level, use the `xs` (\"cross section\") method:\n",
    "df.xs('Erie', level=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Alternatively, you can still use `loc` and pass in an empty slice for the outer level:\n",
    "df.loc[(slice(None), 'Erie'), :]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
